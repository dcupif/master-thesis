Because mankind consumes more and more services made up mostly or entirely of software, software has become deeply entwined in our lives. We put our trust in software in our everyday lives with no idea of how safe it is. The answer is yet quite straight: software is insecure. No matter how innocuous a program could seem, it is a fact: programs written by humans have bugs. Not only identifying and fixing bugs is a very expensive and time-consuming step of the development lifecycle, but insecure software can lead to dreadful consequences when it powers up critical infrastructures. The need for software assurance has surely never been this critical.

Static analysis is, among other options, an automated technique for finding bugs in software. By performing a static examination of the source code, static analyzers are able to identify defects in the source code, therefore guiding the developers' code review in their crusade of removing all bugs in their code. We described in this paper how static analysis internals work, and we came to the conclusion that static analyzers are perfectible by design. Therefore, their effectiveness should be subject to evaluation.

\gls{samate} team at \gls{nist} has already organized five \glspl{sate} with the goal to measure the effectiveness of static analyzers. However, the efforts for evaluating tools have always been hampered by the shortage of realistic, statistically significant and ground-truth corpora. My work as a member of the \gls{samate} team was to develop improved test material for \gls{sate} sixth edition, showcasing the three following properties: ground-truth, relevance, and statistical significance.

After drawing a state of the art on the subject, we realized that attempts to manually create high quality vulnerability corpora have already been undertaken, in vain, unfortunately. Creating such corpora is a difficult and lengthy process. It takes an insane amount of work and time that we cannot afford to lose. Though, other automated alternatives exist to automatically seed vulnerabilities, they enter the game with the disadvantage of generating bugs that are not realistic enough.

Nonetheless, in light of our research, we found \gls{lava}, a \acrlong{lava} software, which has the advantage of performing a dynamic taint analysis on the base program. This analysis extracts very crucial information about the potential \glspl{dua} (attacker-controlled data) and \glspl{atp} (injection points) locations in the source code.

As a consequence, we came up with the following solution proposal. Considering that manual vulnerability addition provides the most realistic bugs, but is extremely time-consuming, why not make use of the dynamic taint analysis from \gls{lava} to make the manual vulnerability seeding process much faster? We are still testing if that method is realistic, but so far, it seems to be a very promising and viable solution.

In the future, we need to make sure that we can make the \gls{lava} software run on other potential test cases. Moreover, the use of \gls{lava} is still limited to C programs only, and focused on the injection of buffer overflows vulnerabilities. Future improvements should ideally involve the support of other languages such as JAVA, and refine the search algorithm for injection points to point out other locations in the source code more likely to host different types of weaknesses.

As a conclusion, it was really worthy to work on such a challenging project. It allowed me to develop a new complete set of skills and I feel confident now about my ability to bring my contribution to a very close-knit team of supportive and skilled co-workers in a computer science research laboratory.

I am really grateful to the \gls{samate} team for welcoming me that well, and giving me all the support I needed for the sucessfull undertaking of my responsibilities on this project.